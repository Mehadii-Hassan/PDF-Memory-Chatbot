{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "collapsed": true,
        "id": "5IegSuuNJVfo"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install langchain langchain-openai faiss-cpu sentence-transformers transformers langchain-core --quiet\n",
        "!pip install langchain_ollama langchain langchain-community langchain-core --quiet\n",
        "!pip install ollama --quiet\n",
        "!pip install pypdf --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "S_NpXA8g82c3"
      },
      "outputs": [],
      "source": [
        "# --- Imports and Setup ---\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "mTLBhNaZM2Qf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# List of PDF files\n",
        "file_paths = [\"/content/Faculty-Name-Short-Form.pdf\", \"/content/IUBAT-Department-Wise-Faculty-Information.pdf\"]\n",
        "\n",
        "# Load all documents\n",
        "docs = []\n",
        "for path in file_paths:\n",
        "    loader = PyPDFLoader(path)\n",
        "    docs.extend(loader.load())  # Append all pages from each PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEGk4pOcM6uK",
        "outputId": "fba0801a-f831-4229-97e9-f5930e2b6ad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 128 chunks\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Split into {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sotURF0WPPlG"
      },
      "outputs": [],
      "source": [
        "# --- 1. OpenAI Model Setup (via GitHub-hosted endpoint) ---\n",
        "# NOTE: Secure your token. Never expose it in production or public code.\n",
        "os.environ['GITHUB_TOKEN'] = \"use_your_api_key\"\n",
        "token = os.environ.get(\"GITHUB_TOKEN\")\n",
        "endpoint = \"https://models.github.ai/inference\"\n",
        "model_name = \"openai/gpt-4.1-nano\"\n",
        "\n",
        "if not token:\n",
        "    raise ValueError(\"GITHUB_TOKEN environment variable not set. Please provide a valid token.\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=model_name,\n",
        "    openai_api_key=token,\n",
        "    openai_api_base=endpoint,\n",
        "    temperature=0.5,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oVgxQz69n0B",
        "outputId": "8ac624bf-4fba-4093-f29a-bc9081f737ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\u001b[0m\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\u001b[0m\r                                                                               \rHit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \u001b[0m\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \u001b[0m\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8OY4yIJtP1iK"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuMTVO6EPE4o",
        "outputId": "ee753a53-6a9b-485e-b4b0-d203d2bfdbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "!ollama pull all-minilm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0pTn71pM8Kj",
        "outputId": "e4edc627-e80c-4a12-9d75-a71a373096cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "I0uJ9ztsM8jD"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "# Use Ollama with all-minilm embedding model (make sure Ollama is running)\n",
        "embedding = OllamaEmbeddings(model=\"all-minilm\")\n",
        "\n",
        "faiss_db = FAISS.from_documents(docs, embedding=embedding)\n",
        "\n",
        "\n",
        "# Save to disk\n",
        "faiss_db.save_local(\"faiss_index\")\n",
        "\n",
        "# Load back\n",
        "db = FAISS.load_local(\"faiss_index\", embedding,allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc4brPrsM-Kh",
        "outputId": "7c8d2ff9-0c0f-487b-9c51-2525c150c348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank_bm25) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "SwKO4jk4M_mj"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "\n",
        "\n",
        "# BM25 Setup\n",
        "texts = [doc.page_content for doc in docs]\n",
        "tokenized_corpus = [t.split(\" \") for t in texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "query = \"What is LangChain?\"\n",
        "bm25_scores = bm25.get_scores(query.split(\" \"))\n",
        "\n",
        "# Get top N BM25 results\n",
        "top_n = 2\n",
        "bm25_ranks = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "bm25_docs = [docs[i[0]] for i in bm25_ranks]\n",
        "\n",
        "# Get top FAISS results\n",
        "faiss_docs = faiss_db.similarity_search(query, k=20)\n",
        "\n",
        "# Merge & deduplicate\n",
        "combined_docs = list({doc.page_content: doc for doc in (bm25_docs + faiss_docs)}.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "QxDXBOyVqP50"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "unique_pages = list({doc.page_content for doc in (bm25_docs + faiss_docs)})\n",
        "combined_docs = [Document(page_content=content, metadata={\"id\": f\"doc_{i}\"}) for i, content in enumerate(unique_pages)]\n",
        "\n",
        "vectorstore = FAISS.from_documents(combined_docs, embedding)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Kgx0t0PvNCF8"
      },
      "outputs": [],
      "source": [
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Given two uploaded PDFs (one containing all university faculty names with their short names, and the other containing faculty names with their mobile numbers, emails, and room numbers) and the latest student query which might reference information from these PDFs, formulate a standalone question that can be understood without needing the PDFs or chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "UVtHNTukSNJT"
      },
      "outputs": [],
      "source": [
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm,retriever , contextualize_q_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "a_pOjTaoWd4M"
      },
      "outputs": [],
      "source": [
        "qa_prompt_with_memory = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\nContext: {context}\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "sUvF7teM2yd4"
      },
      "outputs": [],
      "source": [
        "qa_chain_with_memory = create_stuff_documents_chain(llm, qa_prompt_with_memory)\n",
        "conversational_rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain_with_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Fs7V59N6DpCC"
      },
      "outputs": [],
      "source": [
        "class SessionHistoryManager:\n",
        "    def __init__(self):\n",
        "        self.store = {}\n",
        "\n",
        "    def get_history(self, session_id: str) -> ChatMessageHistory:\n",
        "        if session_id not in self.store:\n",
        "            self.store[session_id] = ChatMessageHistory()\n",
        "        return self.store[session_id]\n",
        "\n",
        "history_manager = SessionHistoryManager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "tKeHHof6XoBv"
      },
      "outputs": [],
      "source": [
        "conversational_chain_with_history = RunnableWithMessageHistory(\n",
        "    conversational_rag_chain,\n",
        "    history_manager.get_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CuEV6DE-VNG",
        "outputId": "6929a504-28ee-4f1c-94aa-196c27cef16d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: Give me the short name of Ms. Imrose Jahan.\n",
            "A: The short name of Ms. Imrose Jahan is IJ.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "res1 = conversational_chain_with_history.invoke(\n",
        "    {\"input\": \"Give me the short name of Ms. Imrose Jahan.\"},\n",
        "    config={\"configurable\": {\"session_id\": \"user123\"}\n",
        "\n",
        "           }\n",
        ")\n",
        "print(\"Q:\", \"Give me the short name of Ms. Imrose Jahan.\")\n",
        "print(\"A:\", res1['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE7trZsbLZ2J",
        "outputId": "0cbf3eeb-ce30-4330-d8c1-061bc29507e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: What was my last question?\n",
            "A: Your last question was: \"Give me the short name of Ms. Imrose Jahan.\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "res2 = conversational_chain_with_history.invoke(\n",
        "    {\"input\": \"What was my last question?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"user123\"}\n",
        "\n",
        "           }\n",
        ")\n",
        "print(\"Q:\", \"What was my last question?\")\n",
        "print(\"A:\", res2['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc6KDu9fI77l",
        "outputId": "757d3796-75d4-41c9-b243-ceabff26b100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: Give me the short name of Dr Md Shariful Islam.\n",
            "A: The short name of Dr Md Shariful Islam is DSI.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "res3 = conversational_chain_with_history.invoke(\n",
        "    {\"input\": \"Give me the short name of Dr Md Shariful Islam.\"},\n",
        "    config={\"configurable\": {\"session_id\": \"user123\"}\n",
        "\n",
        "           }\n",
        ")\n",
        "print(\"Q:\", \"Give me the short name of Dr Md Shariful Islam.\")\n",
        "print(\"A:\", res3['answer'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
